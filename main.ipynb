{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171be79e",
   "metadata": {},
   "source": [
    "# **Parte 1: Infraestrutura**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5b511",
   "metadata": {},
   "source": [
    "\n",
    "- python 3.9+ ✅\n",
    "- miniconda setup ✅\n",
    "- requirements.txt ✅\n",
    "- git público ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc88f71",
   "metadata": {},
   "source": [
    "# **Parte 2: Escolha de base de dados e análise expoloratória**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbabe94",
   "metadata": {},
   "source": [
    "\n",
    "## **Base de Dados**\n",
    "\n",
    "Base de dados escolhida: Titanic - Machine Learning from Disaster\n",
    "Link: https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "## **Objetivo**\n",
    "\n",
    "Realizar análise exploratória sobre dados de passageiros do Titanic à fim de entender as correlações entre sobreviventes e mortos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2295e0",
   "metadata": {},
   "source": [
    "## **Análise Exploratória**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c48fc",
   "metadata": {},
   "source": [
    "### Importação de Bibliotecas e leitura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importação de bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura das bases de dados\n",
    "\n",
    "db = pd.read_csv('train.csv')\n",
    "db.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd4daf",
   "metadata": {},
   "source": [
    "### Análise da Variável Alvo (y): Entendendo a distribuição de Sobrevivência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "contagem_y = db['Survived'].value_counts()\n",
    "\n",
    "porcentagens_y = db['Survived'].value_counts(normalize=True) * 100\n",
    "\n",
    "df_y = pd.DataFrame({\n",
    "    'Contagem': contagem_y,\n",
    "    'Porcentagem': porcentagens_y\n",
    "})\n",
    "df_y.index = ['Não Sobreviveu (0)', 'Sobreviveu (1)']\n",
    "\n",
    "print(\"Tabela de Sobrevivência (Y):\")\n",
    "print(df_y)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=df_y.index, y='Porcentagem', data=df_y, palette=['salmon', 'skyblue'])\n",
    "\n",
    "plt.title('Distribuição da Sobrevivência (Variável Y)', fontsize=14)\n",
    "plt.xlabel('Status de Sobrevivência', fontsize=12)\n",
    "plt.ylabel('Porcentagem de Passageiros (%)', fontsize=12)\n",
    "plt.ylim(0, 100) # Forçar o eixo Y a ir até 100%\n",
    "\n",
    "for i, percentage in enumerate(df_y['Porcentagem']):\n",
    "    plt.text(i, percentage + 2, f'{percentage:.2f}%', ha='center', fontsize=11)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7b12f",
   "metadata": {},
   "source": [
    "O desbalanceamento não é severo (cerca de 62% vs 38%), mas deve ser notado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f70ece",
   "metadata": {},
   "source": [
    "### Análise das Variáveis Explicativas: Entendendo a distribuição de cada feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307d687",
   "metadata": {},
   "source": [
    "#### Gênero vs Sobrevivência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tabela_contagem = pd.crosstab(\n",
    "    index = db['Sex'],       # Linhas: Gênero\n",
    "    columns = db['Survived'] # Colunas: Sobreviveu (0 ou 1)\n",
    ")\n",
    "\n",
    "# Renomear as colunas para melhor leitura\n",
    "tabela_contagem.columns = ['Não Sobreviveu', 'Sobreviveu']\n",
    "\n",
    "print(\"Tabela de Contagem (Gênero x Sobrevivência):\")\n",
    "print(tabela_contagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_porcentagem = pd.crosstab(\n",
    "    index = db['Sex'],\n",
    "    columns = db['Survived'],\n",
    "    normalize = 'index'\n",
    ") * 100 \n",
    "\n",
    "tabela_porcentagem.columns = ['% Não Sobreviveu', '% Sobreviveu']\n",
    "tabela_porcentagem = tabela_porcentagem.round(2)\n",
    "\n",
    "print(\"\\nTabela de Porcentagem (Taxa de Sobrevivência por Gênero):\")\n",
    "print(tabela_porcentagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbb6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Usamos o método 'plot(kind='bar')' na tabela de porcentagem para facilitar\n",
    "tabela_porcentagem['% Sobreviveu'].plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "\n",
    "plt.title('Taxa de Sobrevivência por Gênero', fontsize=16)\n",
    "plt.xlabel('Gênero', fontsize=12)\n",
    "plt.ylabel('Porcentagem de Sobrevivência (%)', fontsize=12)\n",
    "\n",
    "# Rótulos no eixo X\n",
    "plt.xticks(ticks=[0, 1], labels=['Mulher', 'Homem'], rotation=0)\n",
    "\n",
    "# Adicionar a linha do 50% para contexto\n",
    "plt.axhline(50, color='gray', linestyle='--', alpha=0.7, label='50% Sobrevivência')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9cc80",
   "metadata": {},
   "source": [
    "Essa tabela mostra claramente a correlação mais forte no dataset: 74.25% das mulheres sobreviveram, enquanto apenas 18.89% dos homens sobreviveram. A variável Sex é, portanto, a feature mais relevante para a previsão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db524443",
   "metadata": {},
   "source": [
    "#### Portão de Embarque, Tarifa e Classe vs Sobrevivência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagem de valores nulos em 'Embarked'\n",
    "nulos_embarked = db['Embarked'].isnull().sum()\n",
    "print(f\"Número de valores nulos em 'Embarked': {nulos_embarked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffa9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "moda_embarked = db['Embarked'].mode()[0]\n",
    "print(f\"A moda (porto mais comum) é: {moda_embarked}\")\n",
    "\n",
    "# Imputação (Substituição) dos valores nulos\n",
    "db['Embarked'].fillna(moda_embarked, inplace=True)\n",
    "\n",
    "# Verificação (deve retornar 0)\n",
    "print(f\"Número de nulos após imputação: {db['Embarked'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tabela de Porcentagem (Taxa de Sobrevivência por Porto)\n",
    "tabela_porcentagem_embarked = pd.crosstab(\n",
    "    index=db['Embarked'],\n",
    "    columns=db['Survived'],\n",
    "    normalize='index' # Normaliza por linha (Porto de Embarque)\n",
    ") * 100\n",
    "\n",
    "# 2. Renomear e formatar\n",
    "tabela_porcentagem_embarked.columns = ['% Não Sobreviveu', '% Sobreviveu']\n",
    "tabela_porcentagem_embarked = tabela_porcentagem_embarked.round(2)\n",
    "\n",
    "print(\"\\nTaxa de Sobrevivência por Porto de Embarque:\")\n",
    "print(tabela_porcentagem_embarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(db['Embarked'], db['Pclass'], normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb807ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tabela de Porcentagem (Taxa de Sobrevivência por Tarifa)\n",
    "tabela_porcentagem_embarked = pd.crosstab(\n",
    "    index=db['Fare'],\n",
    "    columns=db['Survived'],\n",
    "    normalize='index' \n",
    ") * 100\n",
    "\n",
    "# 2. Renomear e formatar\n",
    "tabela_porcentagem_embarked.columns = ['% Não Sobreviveu', '% Sobreviveu']\n",
    "tabela_porcentagem_embarked = tabela_porcentagem_embarked.round(2)\n",
    "\n",
    "print(\"\\nTaxa de Sobrevivência por Tarifa:\")\n",
    "print(tabela_porcentagem_embarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_fare = db['Fare'].median()\n",
    "db['Fare'].fillna(median_fare, inplace=True)\n",
    "\n",
    "# 2. Configuração e Geração do Histograma\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    data=db, \n",
    "    x='Fare', \n",
    "    hue='Survived', \n",
    "    multiple='stack', \n",
    "    kde=True,         \n",
    "    palette={0: 'salmon', 1: 'skyblue'},\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.title('Distribuição de Tarifa por Status de Sobrevivência (Titanic)', fontsize=16)\n",
    "plt.xlabel('Tarifa', fontsize=12)\n",
    "plt.ylabel('Contagem de Passageiros', fontsize=12)\n",
    "plt.legend(title='Sobreviveu', labels=['Sim (1)', 'Não (0)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b67e9",
   "metadata": {},
   "source": [
    "**Conclusão da Análise:**\n",
    "O DataFrame resultante permite a seguinte observação crucial:\n",
    "\n",
    "Cherbourg (C) tem a taxa de sobrevivência mais alta (55.36%).\n",
    "\n",
    "Queenstown (Q) e Southampton (S) têm taxas de sobrevivência significativamente menores (cerca de 31%-33%).\n",
    "\n",
    "Justificativa para Relevância: A diferença é grande, o que sugere que o porto de embarque é uma feature relevante. Isso é geralmente uma correlação indireta, pois passageiros de Cherbourg (C) tinham uma proporção maior na Primeira Classe (Pclass=1), do que os outros portos. \n",
    "\n",
    "Além disos, tarifas mais altas estão correlacionadas com Pclass, e com maior sobrevivência.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe80b4",
   "metadata": {},
   "source": [
    "#### Idade vs Sobrevivencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = db['Age'].median()\n",
    "db['Age'].fillna(median_age, inplace=True)\n",
    "\n",
    "# 2. Configuração e Geração do Histograma\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histograma com Seaborn, usando 'hue' para separar Sobreviveu (1) e Não Sobreviveu (0)\n",
    "sns.histplot(\n",
    "    data=db, \n",
    "    x='Age', \n",
    "    hue='Survived', \n",
    "    multiple='stack', # Empilha as contagens para visualização clara\n",
    "    kde=True,         # Adiciona a Curva de Estimativa de Densidade (KDE)\n",
    "    palette={0: 'salmon', 1: 'skyblue'},\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.title('Distribuição de Idade por Status de Sobrevivência (Titanic)', fontsize=16)\n",
    "plt.xlabel('Idade', fontsize=12)\n",
    "plt.ylabel('Contagem de Passageiros', fontsize=12)\n",
    "plt.legend(title='Sobreviveu', labels=['Sim (1)', 'Não (0)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf3538",
   "metadata": {},
   "source": [
    "# **Parte 3: Clusterização**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf1299",
   "metadata": {},
   "source": [
    "## Preparação e Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d2f3e",
   "metadata": {},
   "source": [
    "Para que K-Means e DBSCAN funcionem corretamente, todas as features precisam ser numéricas e estar na mesma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore') # Ocultar warnings de imputação/scaling\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 1. Feature Engineering: Extrair Título\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "# 2. Feature Engineering: Tamanho da Família e Sozinho\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "df['IsAlone'] = np.where(df['FamilySize'] == 1, 1, 0)\n",
    "\n",
    "# 3. Selecionar Features para Clusterização (Removendo ID, Nome, Cabine e Survived)\n",
    "features_para_cluster = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'FamilySize', 'IsAlone']\n",
    "X = df[features_para_cluster].copy()\n",
    "\n",
    "# 4. Criação do Pipeline de Pré-processamento\n",
    "# Numéricas: Imputar pela Mediana e Escalar\n",
    "numerical_features = ['Age', 'Fare', 'FamilySize']\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categóricas: Imputar pela Moda e One-Hot Encode\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 5. Combinar transformações\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 6. Aplicar o Pré-processamento e obter o array de dados limpos (X_scaled)\n",
    "X_scaled = preprocessor.fit_transform(X)\n",
    "\n",
    "# O X_scaled agora está pronto para a clusterização.\n",
    "print(f\"Shape do Dataset pronto para clusterização: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60821108",
   "metadata": {},
   "source": [
    "## K-Means: Índice de Silhueta e Escolha do Número de Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d9e58",
   "metadata": {},
   "source": [
    "**Mensuração do Índice de Silhueta**\n",
    "\n",
    "O Índice de Silhueta (Silhouette Score) mede quão similar um objeto é ao seu próprio cluster (coesão) em comparação com outros clusters (separação). \n",
    "O valor varia de -1 a +1:\n",
    "\n",
    "* +1: Indica que a amostra está bem pareada ao seu próprio cluster e distante dos clusters vizinhos.\n",
    "* 0: Indica que a amostra está na fronteira de decisão entre dois clusters.\n",
    "* -1: Indica que a amostra foi atribuída ao cluster errado.\n",
    "\n",
    "Para escolher o $k$ ideal, calculamos a Silhueta média para vários valores de $k$ e escolhemos o $k$ que maximiza o score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Testar k de 2 a 10\n",
    "range_n_clusters = range(2, 11)\n",
    "silhouette_avg = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # 1. Aplicar K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # 2. Calcular o Score de Silhueta\n",
    "    silhouette_avg.append(silhouette_score(X_scaled, cluster_labels))\n",
    "\n",
    "# Plotar o resultado\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_n_clusters, silhouette_avg, marker='o', linestyle='--')\n",
    "plt.title(\"Índice de Silhueta para K-Means\")\n",
    "plt.xlabel(\"Número de Clusters (k)\")\n",
    "plt.ylabel(\"Score Médio de Silhueta\")\n",
    "plt.grid(True)\n",
    "plt.savefig('silhouette_kmeans.png')\n",
    "plt.show()\n",
    "\n",
    "# Determinar o k ótimo:\n",
    "optimal_k = range_n_clusters[np.argmax(silhouette_avg)]\n",
    "max_silhouette = np.max(silhouette_avg)\n",
    "print(f\"O número ótimo de clusters (k) para K-Means é: {optimal_k} (Score: {max_silhouette:.3f})\")\n",
    "\n",
    "# 3. Treinar K-Means com o k ótimo\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_optimal.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05270e",
   "metadata": {},
   "source": [
    "## DBSCAN: Clusterização e Silhueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f5912",
   "metadata": {},
   "source": [
    "O DBSCAN não requer que o número de clusters seja predefinido ($k$).\n",
    "Em vez disso, ele é sensível a dois parâmetros:\n",
    "* $\\epsilon$ (eps): Distância máxima entre as amostras para uma ser considerada vizinha da outra.\n",
    "* min_samples: O número de vizinhos em um raio $\\epsilon$ para que um ponto seja considerado um ponto central (core point).\n",
    "\n",
    "Ajustar $\\epsilon$ é crucial. Tipicamente, $\\epsilon$ é escolhido observando o gráfico k-distância (distância até o $k$-ésimo vizinho)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Escolha de hiperparâmetros (ajustáveis com base em tentativa e erro ou k-distância)\n",
    "eps_value = 0.9  # Típico para dados padronizados (0.8 - 1.2)\n",
    "min_samples_value = 5 \n",
    "\n",
    "# 1. Aplicar DBSCAN\n",
    "dbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Contar o número de clusters encontrados (excluindo ruído = -1)\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"\\nDBSCAN Resultados (eps={eps_value}, min_samples={min_samples_value}):\")\n",
    "print(f\"Número de clusters encontrados: {n_clusters_dbscan}\")\n",
    "print(f\"Número de pontos de ruído (label -1): {n_noise}\")\n",
    "\n",
    "# 2. Calcular o Score de Silhueta (apenas se houver mais de 1 cluster)\n",
    "if n_clusters_dbscan > 1:\n",
    "    # A Silhueta pode ser calculada mesmo com pontos de ruído (label -1)\n",
    "    silhouette_dbscan = silhouette_score(X_scaled, dbscan_labels)\n",
    "    print(f\"Score de Silhueta do DBSCAN: {silhouette_dbscan:.3f}\")\n",
    "else:\n",
    "    silhouette_dbscan = 0\n",
    "    print(\"DBSCAN não formou clusters suficientes para calcular o Score de Silhueta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6295c6",
   "metadata": {},
   "source": [
    "## Outras Medidas de Validação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa005f50",
   "metadata": {},
   "source": [
    "Para complementar a Silhueta (que é uma medida interna), vamos usar mais duas:\n",
    "\n",
    "* **Calinski-Harabasz Index (CH):** Mede a razão entre a dispersão inter-cluster e a dispersão intra-cluster. Valores maiores são melhores.\n",
    "* **Davies-Bouldin Index (DB):** Mede a similaridade média entre clusters, onde a similaridade é a razão entre a distância intra-cluster e a distância entre clusters. Valores menores são melhores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1068f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# 1. K-Means (usando o k ótimo encontrado)\n",
    "ch_kmeans = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
    "db_kmeans = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "\n",
    "# 2. DBSCAN (usando os labels encontrados)\n",
    "# CH e DB requerem mais de um cluster e que o ruído (-1) seja removido ou tratado\n",
    "# Criamos máscaras para remover o ruído do DBSCAN para estas métricas\n",
    "mask = (dbscan_labels != -1)\n",
    "if np.sum(mask) > 0 and len(np.unique(dbscan_labels[mask])) > 1:\n",
    "    ch_dbscan = calinski_harabasz_score(X_scaled[mask], dbscan_labels[mask])\n",
    "    db_dbscan = davies_bouldin_score(X_scaled[mask], dbscan_labels[mask])\n",
    "else:\n",
    "    ch_dbscan = np.nan\n",
    "    db_dbscan = np.nan\n",
    "\n",
    "print(\"\\n--- Medidas de Validação ---\")\n",
    "print(f\"K-Means (k={optimal_k}):\")\n",
    "print(f\"  Calinski-Harabasz Score (Maior é melhor): {ch_kmeans:.2f}\")\n",
    "print(f\"  Davies-Bouldin Score (Menor é melhor): {db_dbscan:.2f}\")\n",
    "print(f\"\\nDBSCAN:\")\n",
    "print(f\"  Calinski-Harabasz Score (Maior é melhor): {ch_dbscan:.2f}\")\n",
    "print(f\"  Davies-Bouldin Score (Menor é melhor): {db_dbscan:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66367e",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd4c59",
   "metadata": {},
   "source": [
    "# **Parte 4: Medidas de Similaridade**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2902e225",
   "metadata": {},
   "source": [
    "## Um determinado problema, apresenta 10 séries temporais distintas. Gostaríamos de agrupá-las em 3 grupos, de acordo com um critério de similaridade, baseado no valor máximo de correlação cruzada entre elas. Descreva em tópicos todos os passos necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc72d0",
   "metadata": {},
   "source": [
    "**Agrupamento por Correlação Cruzada Máxima (Max Cross-Correlation - MCC)**\n",
    "\n",
    "O objetivo é agrupar as 10 séries temporais em 3 clusters ($k=3$), utilizando a MCC como critério de similaridade.\n",
    "\n",
    "1. Pré-processamento e Normalização\n",
    "* Garantir Uniformidade: Assegurar que todas as séries temporais possuam a mesma frequência e o mesmo número de observações.\n",
    "* Normalização: Aplicar a padronização (Z-Score) ou normalização (Min-Max) em todas as séries para que as diferenças de magnitude não influenciem indevidamente a correlação.\n",
    "* Estacionariedade: Remover tendências ou sazonalidade (ex: aplicando diferenciação) se a correlação cruzada for muito sensível à não-estacionariedade dos dados.\n",
    "\n",
    "2. Cálculo da Matriz de Similaridade\n",
    "* Correlação Cruzada: Para cada par de séries temporais ($TS_A$ e $TS_B$), calcular a correlação cruzada ao longo de uma faixa definida de lags (deslocamentos temporais).\n",
    "* Identificação do Máximo (MCC): O critério de similaridade $Similaridade(A, B)$ é definido como o valor máximo da correlação cruzada.\n",
    "* Construção da Matriz de Dissimilaridade: Transformar a similaridade em distância/dissimilaridade (para algoritmos baseados em distância):$$\\text{Dissimilaridade}(A, B) = 1 - \\text{Max}(\\text{Correlação Cruzada}(A, B))$$\n",
    "\n",
    "3. Clusterização e ValidaçãoMatriz de Distâncias: Utilizar a matriz de dissimilaridade calculada no passo anterior.\n",
    "* Algoritmo de Clusterização: Aplicar um algoritmo capaz de trabalhar com matrizes de pré-distância (ex: Clusterização Hierárquica).\n",
    "* Validação:Visualização: Utilizar Dendrogramas (no caso da Hierárquica) para inspecionar visualmente a fusão dos grupos.\n",
    "* Métricas: Aplicar métricas de coesão e separação que sejam robustas, como o Índice de Silhueta, diretamente na matriz de dissimilaridade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b806d4",
   "metadata": {},
   "source": [
    "## Para o problema da questão anterior, indique qual algoritmo de clusterização você usaria. Justifique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9963a5f",
   "metadata": {},
   "source": [
    "**Algoritmo Indicado:** Clusterização Hierárquica (Hierarchical Clustering)\n",
    "\n",
    "**Justificativa:** \n",
    "* Baseado em Distância: O algoritmo hierárquico é robusto e funciona perfeitamente quando alimentado com uma matriz de distância pré-calculada (a matriz de Dissimilaridade da MCC).\n",
    "* Distâncias Não-Euclidianas: Não assume a forma convexa e esférica dos clusters (como o K-Means), o que é vital quando se usa uma métrica de similaridade não-euclidiana como a MCC.\n",
    "* Visualização e Flexibilidade: O dendrograma oferece uma ferramenta de validação visual poderosa, permitindo justificar o corte em $k=3$ grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1b3c8",
   "metadata": {},
   "source": [
    "## Indique um caso de uso para essa solução projetada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f825f",
   "metadata": {},
   "source": [
    "O uso da MCC é ideal para problemas onde a forma do padrão é importante, mas o tempo de ocorrência é variável.\n",
    "\n",
    "**Caso de Uso Sugerido:**\n",
    "Agrupamento de Séries Temporais de Consumo de Energia Diário em Residências.\n",
    "\n",
    "**Cenário:** Identificar perfis de uso semelhantes (ex: \"família que acorda cedo\" vs. \"trabalhadores noturnos\").\n",
    "\n",
    "**Aplicação:** Se uma residência atinge o pico de consumo às 7h e outra atinge um pico semelhante, mas às 8h (um deslocamento de 1 lag), a MCC será alta, agrupando-as no mesmo perfil.\n",
    "\n",
    "**Benefício:** Permite que a concessionária crie 3 grupos de perfis para otimizar preços ou gerenciar a demanda na rede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf5ff7",
   "metadata": {},
   "source": [
    "## Sugira outra estratégia para medir a similaridade entre séries temporais. Descreva em tópicos os passos necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4ad99",
   "metadata": {},
   "source": [
    "A alternativa mais poderosa para medir similaridade em séries temporais, que varia em velocidade e não apenas em deslocamento temporal linear, é o **Dynamic Time Warping (DTW)**.\n",
    "\n",
    "**Estratégia Alternativa: Dynamic Time Warping (DTW)**\n",
    "\n",
    "O DTW calcula a distância de desalinhamento, permitindo que os padrões sejam esticados ou comprimidos no tempo para encontrar o caminho de menor custo entre duas séries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0a42c",
   "metadata": {},
   "source": [
    "**Passos Necessários com DTW:**\n",
    "\n",
    "1. **Cálculo da Distância DTW:** Para cada par de séries temporais ($TS_A$ e $TS_B$), calcular o custo mínimo de alinhamento (a distância DTW).\n",
    "2. **Formação da Matriz de Distâncias:** Construir uma matriz simétrica $10 \\times 10$ onde cada elemento $(i, j)$ é a Distância DTW entre $TS_i$ e $TS_j$.\n",
    "3. **Clusterização:** Aplicar um algoritmo de agrupamento (como a Clusterização Hierárquica ou Particionamento em Torno de Medoids - PAM) diretamente na Matriz de Distância DTW.\n",
    "4. **Validação e Interpretação:** Validar os agrupamentos e interpretar os clusters resultantes com base nas formas típicas de cada grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715e5a0",
   "metadata": {},
   "source": [
    "## Referências"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
